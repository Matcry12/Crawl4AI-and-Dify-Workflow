# ğŸ“ Crawl4AI Project Structure

## Overview
This document describes the organized folder structure of Crawl4AI - an intelligent web crawling and knowledge base management system with Dify integration.

---

## ğŸ“‚ Directory Structure

```
Crawl4AI/
â”œâ”€â”€ main.py                    # ğŸš€ Main entry point - start the application
â”œâ”€â”€ requirements.txt           # ğŸ“¦ Python dependencies
â”œâ”€â”€ .env.example              # ğŸ” Environment variables template
â”‚
â”œâ”€â”€ core/                     # ğŸ§  Core Business Logic
â”‚   â”œâ”€â”€ crawl_workflow.py     # Main crawling workflow orchestration
â”‚   â”œâ”€â”€ content_processor.py  # Content analysis and mode selection
â”‚   â”œâ”€â”€ mode_selector.py      # Dual-mode RAG processing logic
â”‚   â”œâ”€â”€ intelligent_content_analyzer.py  # AI-powered content analysis
â”‚   â””â”€â”€ resilience_utils.py   # Error recovery, retry, circuit breaker
â”‚
â”œâ”€â”€ api/                      # ğŸ”Œ External API Integrations
â”‚   â”œâ”€â”€ dify_api_resilient.py # Dify API client with resilience features
â”‚   â”œâ”€â”€ knowledge_base_manager.py  # KB creation and management
â”‚   â”œâ”€â”€ document_manager.py   # Document CRUD operations
â”‚   â””â”€â”€ metadata_manager.py   # Metadata field management
â”‚
â”œâ”€â”€ ui/                       # ğŸ¨ User Interface
â”‚   â”œâ”€â”€ app.py               # Flask web server
â”‚   â””â”€â”€ templates/           # HTML templates
â”‚       â””â”€â”€ index.html       # Main UI dashboard
â”‚
â”œâ”€â”€ utils/                    # ğŸ› ï¸ Utility Functions
â”‚   â”œâ”€â”€ workflow_config.py   # Configuration management
â”‚   â””â”€â”€ workflow_utils.py    # Helper functions
â”‚
â”œâ”€â”€ tests/                    # ğŸ§ª Test Suite
â”‚   â”œâ”€â”€ test_crawl_workflow.py  # Comprehensive 10-test suite
â”‚   â”œâ”€â”€ quick_test.py        # Fast smoke tests (< 10 sec)
â”‚   â”œâ”€â”€ test_metadata.py     # Metadata functionality tests
â”‚   â”œâ”€â”€ test_single_crawl.py # Single page crawl tests
â”‚   â””â”€â”€ [other test files]   # Legacy/specialized tests
â”‚
â”œâ”€â”€ docs/                     # ğŸ“š Documentation
â”‚   â”œâ”€â”€ README.md            # Main project documentation
â”‚   â”œâ”€â”€ TEST_DOCUMENTATION.md  # Test suite guide
â”‚   â”œâ”€â”€ DEPLOYMENT_GUIDE.md  # Deployment instructions
â”‚   â”œâ”€â”€ INTEGRATION_SUMMARY.md  # Integration overview
â”‚   â”œâ”€â”€ ERROR_RESILIENCE_IMPLEMENTATION_REPORT.md
â”‚   â”œâ”€â”€ INTELLIGENT_DUAL_MODE_RAG_TUTORIAL.md
â”‚   â”œâ”€â”€ UI_INTELLIGENT_MODE_GUIDE.md
â”‚   â””â”€â”€ [other docs]         # Feature-specific guides
â”‚
â”œâ”€â”€ models/                   # ğŸ“Š Data Models & Schemas
â”‚   â””â”€â”€ schemas.py           # Pydantic schemas for extraction
â”‚
â”œâ”€â”€ prompts/                  # ğŸ’¬ LLM Prompts
â”‚   â””â”€â”€ [prompt templates]   # Extraction and analysis prompts
â”‚
â”œâ”€â”€ output/                   # ğŸ“¤ Output Files
â”‚   â”œâ”€â”€ test_results.json    # Test execution results
â”‚   â”œâ”€â”€ crawl_checkpoint.json # Checkpoint for crash recovery
â”‚   â””â”€â”€ failure_queue.json   # Failed URL tracking
â”‚
â”œâ”€â”€ scripts/                  # ğŸ“œ Utility Scripts
â”‚   â””â”€â”€ setup.sh             # Installation & setup script
â”‚
â””â”€â”€ backup_old/              # ğŸ—„ï¸ Deprecated Files
    â”œâ”€â”€ crawl_orchestrator.py  # Old orchestration logic
    â””â”€â”€ workflow_refactored_example.py

```

---

## ğŸš€ Quick Start

### 1. Start the Application
```bash
python main.py
```
Access UI at: http://localhost:5000

### 2. Run Tests
```bash
# Comprehensive test suite (10 tests, ~3-5 min)
python tests/test_crawl_workflow.py

# Quick smoke test (< 10 sec)
python tests/quick_test.py
```

### 3. UI Test Buttons
- **ğŸš€ Quick Test** - Validates core functionality
- **âš¡ Stress Test** - Runs full test suite via UI

---

## ğŸ“¦ Key Components

### Core Workflow (`core/crawl_workflow.py`)
- **Purpose**: Main orchestration logic
- **Features**:
  - Intelligent crawling with duplicate detection
  - Automatic KB categorization
  - Dual-mode RAG processing
  - Error resilience & recovery
- **Imports**: Uses api/ and core/ modules

### Resilience System (`core/resilience_utils.py`)
- **CrawlCheckpoint**: Save/resume crawl state
- **FailureQueue**: Track failed URLs for retry
- **RetryConfig**: Exponential backoff retry logic
- **CircuitBreaker**: Prevent cascade failures

### Content Processor (`core/content_processor.py`)
- **ProcessingMode**: FULL_DOC vs PARAGRAPH
- **Dual-Mode Selection**: Word count or AI-based
- **Token Counting**: Efficient content analysis

### Dify API Client (`api/dify_api_resilient.py`)
- **Features**:
  - Automatic retry with exponential backoff
  - Circuit breaker pattern
  - KB/Document/Metadata operations
  - Parent-child chunking support

### Web UI (`ui/app.py`)
- **Flask Server**: Port 5000
- **Features**:
  - Real-time progress streaming (SSE)
  - Dual-model configuration
  - Knowledge base selection
  - Test execution via UI

---

## ğŸ”„ Import Patterns

All modules use relative imports from project root:

```python
# In core/crawl_workflow.py
from api.dify_api_resilient import ResilientDifyAPI
from core.content_processor import ContentProcessor

# In ui/app.py
from core.crawl_workflow import CrawlWorkflow

# In tests/test_crawl_workflow.py
from core.crawl_workflow import CrawlWorkflow
from core.resilience_utils import CrawlCheckpoint
```

---

## ğŸ§ª Test Suite

### Comprehensive Tests (`tests/test_crawl_workflow.py`)
10 tests covering:
1. âœ… Initialization
2. âœ… KB Creation
3. âœ… Document Naming
4. âœ… Duplicate Detection
5. âœ… Checkpoint System
6. âœ… Failure Queue
7. âœ… Single Page Crawl
8. âœ… Dual-Mode Selection
9. âœ… Metadata Fields
10. âœ… Category Normalization

**Success Rate**: 100% (10/10 passing)

### Quick Test (`tests/quick_test.py`)
Fast validation (< 10 seconds) for:
- Initialization
- Document naming
- Category normalization
- Checkpoint system
- Failure queue

---

## ğŸ¯ Default AI Models

All models default to **Gemini 2.5 Flash Lite** (`gemini/gemini-2.5-flash-lite`):

- **Extraction Model**: Content processing & extraction
- **Naming Model**: KB categorization
- **Analysis Model**: Intelligent content analysis

---

## ğŸ“ Configuration

### Environment Variables (`.env`)
```bash
DIFY_BASE_URL=http://localhost:8088
DIFY_API_KEY=your_dify_api_key
GEMINI_API_KEY=your_gemini_api_key
```

### Key Parameters
- `max_pages`: Maximum pages to crawl
- `max_depth`: Crawl depth limit
- `word_threshold`: Dual-mode switching threshold (default: 4000)
- `enable_dual_mode`: Enable smart mode selection
- `enable_resilience`: Enable retry & recovery

---

## ğŸ—‘ï¸ Deprecated Files

Files in `backup_old/` are kept for reference but not used:
- `crawl_orchestrator.py` - Old orchestration logic
- `workflow_refactored_example.py` - Example code

---

## ğŸ“Š Output Files

### Test Results (`output/test_results.json`)
```json
{
  "timestamp": 23556.792276022,
  "total": 10,
  "passed": 10,
  "failed": 0,
  "results": [...]
}
```

### Checkpoint (`output/crawl_checkpoint.json`)
- Tracks crawl progress
- Enables resume after crash
- Stores processed/pending URLs

### Failure Queue (`output/failure_queue.json`)
- Failed URLs with error messages
- Retry count tracking
- Exportable report

---

## ğŸ”— Dependencies

See `requirements.txt` for full list. Key dependencies:
- `crawl4ai` - Web crawling framework
- `flask` - Web UI server
- `python-dotenv` - Environment management
- `requests` - HTTP client
- `pydantic` - Data validation

---

## ğŸ“ Support

- **Documentation**: See `docs/` folder
- **Issues**: GitHub issues
- **Tests**: Run test suite for validation

---

**Last Updated**: 2025-10-06
**Project Version**: 2.0 (Restructured)
**Test Coverage**: 100% (10/10 tests passing)
